#!/usr/bin/env python3
"""
Script de test de performance pour Learnia Backend
"""
import asyncio
import time
import requests
import json
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Any
import statistics


class PerformanceTester:
    """Classe pour tester les performances de l'API"""

    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.results = []

    def test_endpoint_performance(
        self, 
        endpoint: str, 
        method: str = "GET", 
        data: Dict = None,
        headers: Dict = None,
        num_requests: int = 10
    ) -> Dict[str, Any]:
        """
        Teste les performances d'un endpoint
        
        Args:
            endpoint: URL de l'endpoint
            method: M√©thode HTTP
            data: Donn√©es √† envoyer
            headers: En-t√™tes HTTP
            num_requests: Nombre de requ√™tes √† effectuer
        
        Returns:
            Statistiques de performance
        """
        print(f"üß™ Test de performance: {method} {endpoint}")
        
        response_times = []
        success_count = 0
        error_count = 0
        
        for i in range(num_requests):
            start_time = time.time()
            
            try:
                if method.upper() == "GET":
                    response = requests.get(
                        f"{self.base_url}{endpoint}",
                        headers=headers,
                        timeout=30
                    )
                elif method.upper() == "POST":
                    response = requests.post(
                        f"{self.base_url}{endpoint}",
                        json=data,
                        headers=headers,
                        timeout=30
                    )
                else:
                    raise ValueError(f"M√©thode non support√©e: {method}")
                
                response_time = time.time() - start_time
                response_times.append(response_time)
                
                if response.status_code < 400:
                    success_count += 1
                else:
                    error_count += 1
                    print(f"   ‚ùå Erreur {response.status_code}: {response.text[:100]}")
                
            except Exception as e:
                error_count += 1
                response_time = time.time() - start_time
                response_times.append(response_time)
                print(f"   ‚ùå Exception: {e}")
        
        # Calculer les statistiques
        if response_times:
            stats = {
                "endpoint": endpoint,
                "method": method,
                "total_requests": num_requests,
                "success_count": success_count,
                "error_count": error_count,
                "success_rate": (success_count / num_requests) * 100,
                "avg_response_time": statistics.mean(response_times),
                "min_response_time": min(response_times),
                "max_response_time": max(response_times),
                "median_response_time": statistics.median(response_times),
                "p95_response_time": self._percentile(response_times, 95),
                "p99_response_time": self._percentile(response_times, 99),
            }
        else:
            stats = {
                "endpoint": endpoint,
                "method": method,
                "total_requests": num_requests,
                "success_count": 0,
                "error_count": error_count,
                "success_rate": 0,
                "avg_response_time": 0,
                "min_response_time": 0,
                "max_response_time": 0,
                "median_response_time": 0,
                "p95_response_time": 0,
                "p99_response_time": 0,
            }
        
        self.results.append(stats)
        self._print_stats(stats)
        
        return stats

    def _percentile(self, data: List[float], percentile: int) -> float:
        """Calcule un percentile"""
        if not data:
            return 0
        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile / 100)
        return sorted_data[min(index, len(sorted_data) - 1)]

    def _print_stats(self, stats: Dict[str, Any]):
        """Affiche les statistiques de performance"""
        print(f"   üìä R√©sultats:")
        print(f"      Succ√®s: {stats['success_count']}/{stats['total_requests']} ({stats['success_rate']:.1f}%)")
        print(f"      Temps moyen: {stats['avg_response_time']:.3f}s")
        print(f"      Temps m√©dian: {stats['median_response_time']:.3f}s")
        print(f"      P95: {stats['p95_response_time']:.3f}s")
        print(f"      P99: {stats['p99_response_time']:.3f}s")
        print(f"      Min: {stats['min_response_time']:.3f}s")
        print(f"      Max: {stats['max_response_time']:.3f}s")
        print()

    def test_concurrent_requests(
        self, 
        endpoint: str, 
        method: str = "GET",
        data: Dict = None,
        headers: Dict = None,
        num_requests: int = 50,
        max_workers: int = 10
    ) -> Dict[str, Any]:
        """
        Teste les performances avec des requ√™tes concurrentes
        
        Args:
            endpoint: URL de l'endpoint
            method: M√©thode HTTP
            data: Donn√©es √† envoyer
            headers: En-t√™tes HTTP
            num_requests: Nombre de requ√™tes √† effectuer
            max_workers: Nombre de threads concurrents
        
        Returns:
            Statistiques de performance
        """
        print(f"üöÄ Test de performance concurrente: {method} {endpoint}")
        print(f"   Requ√™tes: {num_requests}, Workers: {max_workers}")
        
        def make_request():
            start_time = time.time()
            try:
                if method.upper() == "GET":
                    response = requests.get(
                        f"{self.base_url}{endpoint}",
                        headers=headers,
                        timeout=30
                    )
                elif method.upper() == "POST":
                    response = requests.post(
                        f"{self.base_url}{endpoint}",
                        json=data,
                        headers=headers,
                        timeout=30
                    )
                else:
                    raise ValueError(f"M√©thode non support√©e: {method}")
                
                response_time = time.time() - start_time
                return {
                    "success": response.status_code < 400,
                    "response_time": response_time,
                    "status_code": response.status_code
                }
            except Exception as e:
                response_time = time.time() - start_time
                return {
                    "success": False,
                    "response_time": response_time,
                    "error": str(e)
                }
        
        # Ex√©cuter les requ√™tes concurrentes
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(make_request) for _ in range(num_requests)]
            results = [future.result() for future in futures]
        
        total_time = time.time() - start_time
        
        # Analyser les r√©sultats
        response_times = [r["response_time"] for r in results]
        success_count = sum(1 for r in results if r["success"])
        error_count = num_requests - success_count
        
        stats = {
            "endpoint": endpoint,
            "method": method,
            "total_requests": num_requests,
            "max_workers": max_workers,
            "total_time": total_time,
            "requests_per_second": num_requests / total_time,
            "success_count": success_count,
            "error_count": error_count,
            "success_rate": (success_count / num_requests) * 100,
            "avg_response_time": statistics.mean(response_times),
            "min_response_time": min(response_times),
            "max_response_time": max(response_times),
            "median_response_time": statistics.median(response_times),
            "p95_response_time": self._percentile(response_times, 95),
            "p99_response_time": self._percentile(response_times, 99),
        }
        
        self.results.append(stats)
        self._print_concurrent_stats(stats)
        
        return stats

    def _print_concurrent_stats(self, stats: Dict[str, Any]):
        """Affiche les statistiques de performance concurrente"""
        print(f"   üìä R√©sultats concurrents:")
        print(f"      Temps total: {stats['total_time']:.3f}s")
        print(f"      Requ√™tes/seconde: {stats['requests_per_second']:.2f}")
        print(f"      Succ√®s: {stats['success_count']}/{stats['total_requests']} ({stats['success_rate']:.1f}%)")
        print(f"      Temps moyen: {stats['avg_response_time']:.3f}s")
        print(f"      P95: {stats['p95_response_time']:.3f}s")
        print(f"      P99: {stats['p99_response_time']:.3f}s")
        print()

    def run_comprehensive_test(self):
        """Ex√©cute une suite compl√®te de tests de performance"""
        print("üöÄ Test de performance complet - Learnia Backend")
        print("=" * 60)
        
        # Test 1: Sant√© du backend
        print("1. Test de sant√© du backend")
        self.test_endpoint_performance("/health", "GET", num_requests=5)
        
        # Test 2: Endpoints d'authentification
        print("2. Test des endpoints d'authentification")
        
        # Test d'inscription
        register_data = {
            "email": f"perf_test_{int(time.time())}@example.com",
            "username": f"perf_test_{int(time.time())}",
            "full_name": "Performance Test User",
            "password": "TestPassword123!",
            "grade_level": "Coll√®ge"
        }
        
        self.test_endpoint_performance(
            "/api/v1/auth/register", 
            "POST", 
            data=register_data,
            num_requests=5
        )
        
        # Test de connexion
        login_data = {
            "email": register_data["email"],
            "password": register_data["password"]
        }
        
        login_stats = self.test_endpoint_performance(
            "/api/v1/auth/login", 
            "POST", 
            data=login_data,
            num_requests=5
        )
        
        # Test 3: Endpoints authentifi√©s (si la connexion a r√©ussi)
        if login_stats["success_count"] > 0:
            print("3. Test des endpoints authentifi√©s")
            
            # R√©cup√©rer le token (simulation)
            # En r√©alit√©, il faudrait parser la r√©ponse de login
            token = "test_token"  # Placeholder
            
            headers = {"Authorization": f"Bearer {token}"}
            
            # Test du profil utilisateur
            self.test_endpoint_performance(
                "/api/v1/auth/me", 
                "GET", 
                headers=headers,
                num_requests=5
            )
            
            # Test du tuteur intelligent
            tutor_data = {
                "question": "Qu'est-ce qu'une fraction ?",
                "subject": "Math√©matiques",
                "grade_level": "Coll√®ge"
            }
            
            self.test_endpoint_performance(
                "/api/v1/ai/tutor/", 
                "POST", 
                data=tutor_data,
                headers=headers,
                num_requests=5
            )
        
        # Test 4: Tests de charge
        print("4. Tests de charge")
        
        # Test de charge sur l'endpoint de sant√©
        self.test_concurrent_requests(
            "/health", 
            "GET", 
            num_requests=100,
            max_workers=20
        )
        
        # Test de charge sur l'endpoint de connexion
        self.test_concurrent_requests(
            "/api/v1/auth/login", 
            "POST", 
            data=login_data,
            num_requests=50,
            max_workers=10
        )
        
        # R√©sum√© des r√©sultats
        self.print_summary()

    def print_summary(self):
        """Affiche un r√©sum√© des r√©sultats"""
        print("\n" + "=" * 60)
        print("üìä R√âSUM√â DES PERFORMANCES")
        print("=" * 60)
        
        if not self.results:
            print("Aucun r√©sultat √† afficher.")
            return
        
        # Calculer les moyennes globales
        total_requests = sum(r["total_requests"] for r in self.results)
        total_success = sum(r["success_count"] for r in self.results)
        avg_response_time = statistics.mean([r["avg_response_time"] for r in self.results])
        
        print(f"Total des requ√™tes: {total_requests}")
        print(f"Taux de succ√®s global: {(total_success/total_requests)*100:.1f}%")
        print(f"Temps de r√©ponse moyen: {avg_response_time:.3f}s")
        
        print("\nüìà D√©tails par endpoint:")
        for result in self.results:
            print(f"  {result['method']} {result['endpoint']}:")
            print(f"    Succ√®s: {result['success_count']}/{result['total_requests']} ({result['success_rate']:.1f}%)")
            print(f"    Temps moyen: {result['avg_response_time']:.3f}s")
            if 'requests_per_second' in result:
                print(f"    Requ√™tes/seconde: {result['requests_per_second']:.2f}")
        
        print("\n‚úÖ Test de performance termin√©!")


def main():
    """Fonction principale"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Test de performance Learnia Backend")
    parser.add_argument("--url", default="http://localhost:8000", help="URL du backend")
    parser.add_argument("--endpoint", help="Endpoint sp√©cifique √† tester")
    parser.add_argument("--requests", type=int, default=10, help="Nombre de requ√™tes")
    parser.add_argument("--concurrent", action="store_true", help="Test concurrent")
    parser.add_argument("--workers", type=int, default=10, help="Nombre de workers concurrents")
    
    args = parser.parse_args()
    
    tester = PerformanceTester(args.url)
    
    if args.endpoint:
        # Test d'un endpoint sp√©cifique
        if args.concurrent:
            tester.test_concurrent_requests(
                args.endpoint, 
                num_requests=args.requests,
                max_workers=args.workers
            )
        else:
            tester.test_endpoint_performance(
                args.endpoint, 
                num_requests=args.requests
            )
    else:
        # Test complet
        tester.run_comprehensive_test()


if __name__ == "__main__":
    main()
